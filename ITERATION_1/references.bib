@article{DATTA2024104762,
    title = {ESMA: Towards elevating system happiness in a decentralized serverless edge computing framework},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {183},
    pages = {104762},
    year = {2024},
    issn = {0743-7315},
    doi = {https://doi.org/10.1016/j.jpdc.2023.104762},
    url = {https://www.sciencedirect.com/science/article/pii/S0743731523001326},
    author = {Somoshree Datta and Sourav Kanti Addya and Soumya K. Ghosh},
    keywords = {Edge computing, Serverless computing, Matching theory},
    abstract = {Due to the rapid growth in the adoption of numerous technologies, such as smartphones and the Internet of Things (IoT), edge and serverless computing have started gaining momentum in today's computing infrastructure. It has led to the production of huge amounts of data and has also resulted in increased network traffic, which if not managed well can cause network congestion. To address this and maintain the quality of service (QoS), in this work, a novel dispatch (destination selection) algorithm called Egalitarian Stable Matching Algorithm (ESMA) for faster data processing has been developed while also considering the best use of server resources in a decentralized Serverless-Edge environment. This will allow us to effectively utilize the enormous volumes of data that are generated. The proposed algorithm has been able to achieve lower overall dissatisfaction scores for the entire system. Individually, the client's happiness as well as the server's happiness have improved over the baseline. Moreover, there has been a drop of 25.7% in the total execution time and the total network resources consumed are lower as compared to the baseline algorithm as well as random-allocation algorithm.}
}
@article{UNLU2024107334,
    title = {Microservice-based projects in agile world: A structured interview},
    journal = {Information and Software Technology},
    volume = {165},
    pages = {107334},
    year = {2024},
    issn = {0950-5849},
    doi = {https://doi.org/10.1016/j.infsof.2023.107334},
    url = {https://www.sciencedirect.com/science/article/pii/S0950584923001157},
    author = {Hüseyin Ünlü and Dhia Eddine Kennouche and Görkem Kılınç Soylu and Onur Demirörs},
    keywords = {Microservices, Agile software development, DevOps, Software architecture, Project management},
    abstract = {In this article, we investigate the dynamics of using microservice architecture in the context of agile development. We conducted structured interviews with industry professionals to gather insights into how microservices can improve scalability and maintainability, and the challenges associated with their adoption in an agile project setting.}
}

@article{GIMENOSARROCA2024104764,
    title = {MLLess: Achieving cost efficiency in serverless machine learning training},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {183},
    pages = {104764},
    year = {2024},
    issn = {0743-7315},
    doi = {https://doi.org/10.1016/j.jpdc.2023.104764},
    url = {https://www.sciencedirect.com/science/article/pii/S074373152300134X},
    author = {Pablo {Gimeno Sarroca} and Marc Sánchez-Artigas},
    keywords = {Serverless computing, Function-as-a-Service, Machine learning},
    abstract = {Function-as-a-Service (FaaS) has raised a growing interest in how to “tame” serverless computing to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional “serverful” computing. To help in this endeavor, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two innovative optimizations tailored to the traits of serverless computing: on one hand, a significance filter, to make indirect communication more effective, and on the other hand, a scale-in auto-tuner, to reduce cost by benefiting from the FaaS sub-second billing model (often per 100 ms). Our results certify that MLLess can be 15X faster than serverful ML systems at a lower cost for sparse ML models that exhibit fast convergence such as sparse logistic regression and matrix factorization. Furthermore, our results show that MLLess can easily scale out to increasingly large fleets of serverless workers.}
}

@article{LI2024104781,
    title = {Scheduling independent tasks on multiple cloud-assisted edge servers with energy constraint},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {184},
    pages = {104781},
    year = {2024},
    issn = {0743-7315},
    doi = {https://doi.org/10.1016/j.jpdc.2023.104781},
    url = {https://www.sciencedirect.com/science/article/pii/S074373152300151X},
    author = {Keqin Li},
    keywords = {Asymptotic performance bound, Cloud-assisted edge server, Effective speed, Energy constraint, Heuristic algorithm, Mobile edge computing, Task scheduling},
    abstract = {In this paper, we study task scheduling with or without energy constraint in mobile edge computing with multiple cloud-assisted edge servers as combinatorial optimization problems within the framework of classical scheduling theory. The first problem is to schedule a list of independent tasks on a mobile device and several heterogeneous edge servers and cloud servers, such that the makespan is minimized. The second problem is to schedule a list of independent tasks and to determine the computation and communication speeds of a mobile device, such that the makespan is minimized and the energy consumption of the mobile device does not exceed certain energy budget. The paper makes the following tangible contributions. We design heuristic task scheduling algorithms for both problems by considering the heterogeneity of computation and communication speeds. We derive a lower bound for the optimal schedule and prove an asymptotic performance bound for our heuristic algorithms. We experimentally evaluate the performance of our heuristic algorithms and show that their performance is very close to that of an optimal algorithm.}
}

@article{ROSENDO202271,
    title = {Distributed intelligence on the Edge-to-Cloud Continuum: A systematic literature review},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {166},
    pages = {71-94},
    year = {2022},
    issn = {0743-7315},
    doi = {https://doi.org/10.1016/j.jpdc.2022.04.004},
    url = {https://www.sciencedirect.com/science/article/pii/S0743731522000843},
    author = {Daniel Rosendo and Alexandru Costan and Patrick Valduriez and Gabriel Antoniu},
    keywords = {Edge computing, Distributed intelligence, Big Data Analytics, Computing Continuum, Reproducibility},
    abstract = {The explosion of data volumes generated by an increasing number of applications is strongly impacting the evolution of distributed digital infrastructures for data analytics and machine learning (ML). While data analytics used to be mainly performed on cloud infrastructures, the rapid development of IoT infrastructures and the requirements for low-latency, secure processing has motivated the development of edge analytics. Today, to balance various trade-offs, ML-based analytics tends to increasingly leverage an interconnected ecosystem that allows complex applications to be executed on hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC systems in what is called the Computing Continuum, the Digital Continuum, or the Transcontinuum. Enabling learning-based analytics on such complex infrastructures is challenging. The large scale and optimized deployment of learning-based workflows across the Edge-to-Cloud Continuum requires extensive and reproducible experimental analysis of the application execution on representative testbeds. This is necessary to help understand the performance trade-offs that result from combining a variety of learning paradigms and supportive frameworks.}
}
